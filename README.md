# Hallucination-Detection-in-LLM-s
The proliferation of large language models (LLMs) has raised concerns about the accuracy of their outputs, particularly regarding hallucinationsâ€”instances where models generate factually incorrect or misleading information. This project tackles the challenge of hallucination detection by training multiple transformer-based models, including DistilBERT, BERT, RoBERTa, and DeBERTa, on the TruthfulQA dataset. These models are designed to classify answers as either truthful or hallucinated, with performance evaluated using accuracy, precision, recall, and F1-score metrics. 

In addition to the traditional classification approach, the project introduces a knowledge-based method involving text generation with GPT-2 and Wikipedia-based evidence retrieval. By comparing the semantic similarity between generated answers and retrieved Wikipedia summaries using Sentence-BERT, answers are flagged as hallucinated if they deviate significantly from the evidence. The hybrid approach of combining model-based classification with external knowledge sources provides a more robust solution for detecting hallucinations in LLMs, enhancing their reliability for real-world applications. 
