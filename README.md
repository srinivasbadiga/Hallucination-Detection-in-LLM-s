# Hallucination-Detection-in-LLM-s
The proliferation of large language models (LLMs) has raised concerns about the accuracy of their outputs, particularly regarding hallucinations—instances where models generate factually incorrect or misleading information. This project tackles the challenge of hallucination detection by training multiple transformer-based models, including DistilBERT, BERT, RoBERTa, and DeBERTa, on the TruthfulQA dataset. These models are designed to classify answers as either truthful or hallucinated, with performance evaluated using accuracy, precision, recall, and F1-score metrics. 

In addition to the traditional classification approach, the project introduces a knowledge-based method involving text generation with GPT-2 and Wikipedia-based evidence retrieval. By comparing the semantic similarity between generated answers and retrieved Wikipedia summaries using Sentence-BERT, answers are flagged as hallucinated if they deviate significantly from the evidence. The hybrid approach of combining model-based classification with external knowledge sources provides a more robust solution for detecting hallucinations in LLMs, enhancing their reliability for real-world applications. 

## AI Model Attribution

This project uses language models provided by [OpenAI](https://openai.com), such as ChatGPT/GPT-4.

Some outputs or components may be generated or assisted by OpenAI's models via the [ChatGPT API](https://platform.openai.com/docs/guides/gpt).

OpenAI's models are governed by their [terms of use](https://openai.com/policies/terms-of-use) and [usage policies](https://openai.com/policies/usage-policies).

All rights to the model outputs generated by the API are subject to OpenAI’s [Ownership](https://openai.com/policies/terms-of-use#ownership) terms.

Some parts of this project (e.g., code comments, text summaries) were generated or enhanced with the help of ChatGPT.

